
# Choose device
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained('malaysia-ai/Malaysian-Llama-3.2-3B-Instruct')
streamer = TextStreamer(tokenizer)
model = AutoModelForCausalLM.from_pretrained(
    'malaysia-ai/Malaysian-Llama-3.2-3B-Instruct', torch_dtype = torch.bfloat16
).cuda()



# Create text generation pipeline
text_generation_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if device == "cuda" else -1,
    max_length=1000,
    truncation=True,
    streamer=streamer,
    do_sample=True,
)

# Prompt user for input
while True:
    user_input = input("\nüìù Enter any message (or type 'exit' to quit): ")
    if user_input.lower() == 'exit':
        print("üëã Goodbye!")
        break

    text_header = (f"You are now a navigation assistant, you will answer driver's question short and simple. Answer the following "
                   f"request from the driver: ")
    location_text = f".The driver current location is: latitude {lat}, longitude {lng}."
    command_tune = f"Reply it naturally like a human response."


    response = text_generation_pipeline(f"{text_header} Q:{user_input}\nA:{location_text}.")
    # print(response)
    print("\nü§ñ Response:")
    print(response[0]['generated_text'])
